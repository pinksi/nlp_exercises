{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d01c440-7055-4437-8ce9-a5e8ffef9998",
   "metadata": {},
   "source": [
    "#### Testing spacy and nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "077bb537-dbbc-4621-863d-7b30399cb619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91bd2970-ccea-46ce-a474-bf4d2fbc9ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like to read.\n",
      "I\n",
      "like\n",
      "to\n",
      "read\n",
      ".\n",
      "I enjoy hiking.\n",
      "I\n",
      "enjoy\n",
      "hiking\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"I like to read. I enjoy hiking.\")\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)\n",
    "    for word in sentence:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70f85964-4d9c-4f03-9123-02def6ef48c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e08df23-a5b2-47e6-9178-9ad3e7e7c0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/pinkysitikhu/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9d4f27d-4376-4671-9726-7d377090fc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dr. Pinky like to read.', 'I enjoy hiking.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Dr.', 'Pinky', 'like', 'to', 'read', '.', 'I', 'enjoy', 'hiking', '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "print(sent_tokenize(\"Dr. Pinky like to read. I enjoy hiking.\"))\n",
    "word_tokenize(\"Dr. Pinky like to read. I enjoy hiking.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6a6319-c3aa-49eb-843e-449965dec861",
   "metadata": {},
   "source": [
    "#### Sentence Tokenization using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50fbf443-0900-4fcd-98b8-c8687570ac63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "Pinky\n",
      "like\n",
      "to\n",
      "read\n",
      ".\n",
      "I\n",
      "enjoy\n",
      "hiking\n",
      ".\n",
      "Let\n",
      "'s\n",
      "go\n",
      "to\n",
      "N.Y\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(\"Dr. Pinky like to read. I enjoy hiking. Let's go to N.Y!\")\n",
    "for word in doc:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f0c0d0-a3b2-473b-9558-47e692ea16af",
   "metadata": {},
   "source": [
    "#### Add special case in tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6405ef9-de2a-48f8-9d0b-4fdecbc5c196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gim\n",
      "me\n",
      "that\n",
      "[{65: 'gim'}, {65: 'me'}]\n",
      "gim\n",
      "me\n",
      "that\n"
     ]
    }
   ],
   "source": [
    "from spacy.symbols import ORTH\n",
    "\n",
    "text = \"gimme that\"\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "\n",
    "special_case = [{ORTH:\"gim\"}, {ORTH:\"me\"}]\n",
    "print(special_case)\n",
    "nlp.tokenizer.add_special_case(\"gimme\", special_case)\n",
    "for token in doc:\n",
    "    print(token.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161f2afb-3878-4dfe-8c10-be3bd4fdc06d",
   "metadata": {},
   "source": [
    "#### Extract all urls from the text below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f4c7d47-e325-4aa6-b26e-caffe577a683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://www.data.gov/', 'http://www.science', 'http://data.gov.uk/.', 'http://www3.norc.org/gss+website/', 'http://www.europeansocialsurvey.org/.']\n"
     ]
    }
   ],
   "source": [
    "text='''\n",
    "Look for data to help you address the question. Governments are good\n",
    "sources because data from public research is often freely available. Good\n",
    "places to start include http://www.data.gov/, and http://www.science.\n",
    "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
    "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \n",
    "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
    "'''\n",
    "doc = nlp(text)\n",
    "urls = []\n",
    "for word in doc:\n",
    "    if word.like_url:\n",
    "        urls.append(word.text)\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e681665f-4b81-4153-bc8c-d7f82d9e1a7f",
   "metadata": {},
   "source": [
    "#### Extract all money values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55b3fae7-38fd-4a5c-8348-45c96ad0d473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "transactions = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\"\n",
    "doc = nlp(transactions)\n",
    "for i, word in enumerate(doc):\n",
    "    if word.is_currency:\n",
    "        print(doc[i-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39cc3008-d962-404c-ab38-d1f55cd3c6d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2dc84b-24e7-4f75-9926-619c44309ec5",
   "metadata": {},
   "source": [
    "#### If you initiate spacy as blank, tokenizer will be there already. Other packages can be added in the spacy pipeline.\n",
    "\n",
    "`en_core_web_sm` --> one of the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "819d710d-3fb0-4144-bc48-a24149c1c0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "Dr.  |  PROPN | Dr.\n",
      "Pinky  |  PROPN | Pinky\n",
      "like  |  VERB | like\n",
      "to  |  PART | to\n",
      "read  |  VERB | read\n",
      ".  |  PUNCT | .\n",
      "I  |  PRON | I\n",
      "enjoy  |  VERB | enjoy\n",
      "hiking  |  VERB | hike\n",
      ".  |  PUNCT | .\n",
      "Let  |  VERB | let\n",
      "'s  |  PRON | us\n",
      "go  |  VERB | go\n",
      "to  |  ADP | to\n",
      "N.Y  |  PROPN | N.Y\n",
      "and  |  CCONJ | and\n",
      "San  |  PROPN | San\n",
      "Francisco  |  PROPN | Francisco\n",
      "!  |  PUNCT | !\n",
      "I  |  PRON | I\n",
      "have  |  VERB | have\n",
      "$  |  SYM | $\n",
      "4  |  NUM | 4\n",
      "money  |  NOUN | money\n",
      ".  |  PUNCT | .\n",
      "Pinky | PERSON | People, including fictional\n",
      "N.Y | GPE | Countries, cities, states\n",
      "San Francisco | GPE | Countries, cities, states\n",
      "4 | MONEY | Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "print(nlp.pipe_names)\n",
    "doc = nlp(\"Dr. Pinky like to read. I enjoy hiking. Let's go to N.Y and San Francisco! I have $4 money.\")\n",
    "for token in doc:\n",
    "    print(token, \" | \", token.pos_, \"|\", token.lemma_)\n",
    "    \n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"|\", ent.label_, \"|\", spacy.explain(ent.label_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "942b0d61-b548-48cd-b07d-2ea24b99d6c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Dr. \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Pinky\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " like to read. I enjoy hiking. Let's go to \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    N.Y\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    San Francisco\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "! I have $\n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    4\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       " money.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for nice visualization\n",
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28a0ea72-31d3-4204-9034-e892d8ce9304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Bloomberg\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " founded \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    bloomberg\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1990\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = \"Bloomberg founded bloomberg in 1990\"\n",
    "doc = nlp(t)\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1075e3f3-1aa0-44fd-bc34-d32fe9331ed9",
   "metadata": {},
   "source": [
    "#### Using blank pipeline and add components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fbf810e-414f-4f85-b902-9a1d79071481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ner']\n",
      "-----------\n",
      "Bloomberg | PERSON | People, including fictional\n",
      "bloomberg | ORG | Companies, agencies, institutions, etc.\n",
      "1990 | DATE | Absolute or relative dates or periods\n",
      "-----------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Bloomberg\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " founded \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    bloomberg\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1990\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main_pipe = spacy.load(\"en_core_web_sm\")\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"ner\", source=main_pipe)\n",
    "print(nlp.pipe_names)\n",
    "print(\"-----------\")\n",
    "t = \"Bloomberg founded bloomberg in 1990\"\n",
    "doc = nlp(t)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"|\", ent.label_, \"|\", spacy.explain(ent.label_))\n",
    "print(\"-----------\")\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fcb431-cc11-400b-b03e-4c227117aad6",
   "metadata": {},
   "source": [
    "#### Lemmatization in spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09e882c9-226f-4bc7-89fb-6e0112e98d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. -> Dr.\n",
      "Pinky -> Pinky\n",
      "like -> like\n",
      "reading -> read\n",
      ". -> .\n",
      "I -> I\n",
      "enjoy -> enjoy\n",
      "hiking -> hike\n",
      ". -> .\n",
      "Let -> let\n",
      "'s -> us\n",
      "go -> go\n",
      "to -> to\n",
      "clubbing -> club\n",
      "! -> !\n",
      "I -> I\n",
      "have -> have\n",
      "$ -> $\n",
      "4 -> 4\n",
      "money -> money\n",
      ". -> .\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Dr. Pinky like reading. I enjoy hiking. Let's go to clubbing! I have $4 money.\")\n",
    "for token in doc:\n",
    "    print(token, \"->\", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eae376ec-af49-45d0-b805-40ba85a3f116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4151936-7677-4fa0-84fc-b4e023477450",
   "metadata": {},
   "source": [
    "#### add custom attribute/lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4987047-ec27-4afd-be91-4860c44a6c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gimme -> Give me\n",
      "some -> some\n",
      "sunshine -> sunshine\n",
      ". -> .\n",
      "It -> it\n",
      "is -> be\n",
      "raining -> rain\n"
     ]
    }
   ],
   "source": [
    "att_ruler = nlp.get_pipe(\"attribute_ruler\")\n",
    "att_ruler.add([[{\"TEXT\":\"gimme\"}]], {\"LEMMA\": \"Give me\"})\n",
    "doc = nlp(\"gimme some sunshine. It is raining\")\n",
    "for token in doc:\n",
    "    print(token.text, \"->\", token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb5d7e3-243f-4c7e-aaad-3f114a6b646f",
   "metadata": {},
   "source": [
    "#### Part of Speech: PoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b80911ea-e38c-4a78-8671-9acee8f6068f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. | PROPN | proper noun | NNP | noun, proper singular\n",
      "Pinky | PROPN | proper noun | NNP | noun, proper singular\n",
      "like | ADP | adposition | IN | conjunction, subordinating or preposition\n",
      "reading | VERB | verb | VBG | verb, gerund or present participle\n",
      ". | PUNCT | punctuation | . | punctuation mark, sentence closer\n",
      "I | PRON | pronoun | PRP | pronoun, personal\n",
      "enjoy | VERB | verb | VBP | verb, non-3rd person singular present\n",
      "hiking | VERB | verb | VBG | verb, gerund or present participle\n",
      "! | PUNCT | punctuation | . | punctuation mark, sentence closer\n",
      "Shall | AUX | auxiliary | MD | verb, modal auxiliary\n",
      "we | PRON | pronoun | PRP | pronoun, personal\n",
      "go | VERB | verb | VB | verb, base form\n",
      "? | PUNCT | punctuation | . | punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Dr. Pinky like reading. I enjoy hiking! Shall we go?\")\n",
    "for token in doc:\n",
    "    print(token, \"|\", token.pos_, \"|\", spacy.explain(token.pos_), \"|\", token.tag_, \"|\", spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c553c325-63ab-43fc-876a-2b89cad7b2fb",
   "metadata": {},
   "source": [
    "#### Count number of POS tag in a doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e91175c-5533-4ee4-8992-95db5584a779",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{96: 2, 85: 1, 100: 4, 97: 3, 95: 2, 87: 1}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = doc.count_by(spacy.attrs.POS)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b55e641-eaee-47a6-b024-6bf9f8205288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('PROPN', 'ADP')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.vocab[96].text, doc.vocab[85].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "29b9a7e2-4432-4185-ac85-5a3c5f93e19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROPN -> 2\n",
      "ADP -> 1\n",
      "VERB -> 4\n",
      "PUNCT -> 3\n",
      "PRON -> 2\n",
      "AUX -> 1\n"
     ]
    }
   ],
   "source": [
    "for k, v in count.items():\n",
    "    print(doc.vocab[k].text, \"->\", v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6747fe1c-484c-4ee2-806e-b0a59ec23ebe",
   "metadata": {},
   "source": [
    "#### Extract all NOUN tokens and NUM POS type, and count all POS tags in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3e066fd7-7ef7-4465-bf67-135bb1c584a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''At its core, machine learning is not a difficult concept to grasp. In fact, the vast majority of machine learning \n",
    "algorithms are concerned with just one simple task: drawing lines. In particular, \n",
    "machine learning is all about drawing lines through data. What does that mean? Let’s look at a simple example. Inflation \n",
    "rose again in April, continuing a climb that has pushed consumers to the brink and is threatening the economic expansion, the \n",
    "Bureau of Labor Statistics reported Wednesday.\\n\\nThe consumer price index, a broad-based measure of prices for goods \n",
    "and services, increased 8.3% from a year ago, higher than the Dow Jones estimate for an 8.1% gain.'''\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7b0a6faa-e4ad-48a3-9d91-26ed1352b87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN [core, machine, learning, concept, fact, majority, machine, learning, algorithms, task, lines, machine, learning, lines, data, ’s, example, Inflation, climb, consumers, brink, expansion, consumer, price, index, measure, prices, goods, services, %, year, estimate, %, gain]\n",
      "NUM [one, 8.3, 8.1]\n",
      "expansion - ADP : 16\n",
      ". - PRON : 4\n",
      "Statistics - NOUN : 34\n",
      "The - PUNCT : 17\n",
      "the - AUX : 6\n",
      "Wednesday - PART : 2\n",
      "of - DET : 12\n",
      "economic - ADJ : 7\n",
      "index - VERB : 14\n",
      "broad - SPACE : 6\n",
      ", - ADV : 5\n",
      "reported - NUM : 3\n",
      "\n",
      "\n",
      " - PROPN : 7\n",
      "Bureau - CCONJ : 2\n"
     ]
    }
   ],
   "source": [
    "noun_token = []\n",
    "num_token = []\n",
    "for token in doc:\n",
    "    if token.pos_ == \"NOUN\":\n",
    "        noun_token.append(token)\n",
    "    elif token.pos_ == \"NUM\":\n",
    "        num_token.append(token)\n",
    "        \n",
    "print(\"NOUN\", noun_token)\n",
    "print(\"NUM\", num_token)\n",
    "count = doc.count_by(spacy.attrs.POS)\n",
    "for k, v in count.items():\n",
    "    print(doc[k], \"-\", doc.vocab[k].text, \":\", v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3d4734-ac2b-4007-8099-76e989c6a7b4",
   "metadata": {},
   "source": [
    "#### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7b792ae-96dd-462b-afca-f407412f9a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4056909-15a2-4068-b6d6-cede3432ad2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seeming', 'behind', 'moreover', 'and', 'get', 'everything', 'before', 'has', 'since', 'at', 'quite', 'noone', 'was', 'however', 'anyone', 'his', 'than', 'therefore', 'whom', 'such', 'thence', 'though', 'from', 'an', 'around', 'had', 'name', 'seems', 'ca', 'mine', 'she', 'whereas', 'by', \"'d\", 'he', 'otherwise', 'afterwards', 'beside', 'a', 'front', 'how', 'were', 'we', 'is', 'please', 'more', 'across', 'ours', 'per', 'among', 'her', 'whatever', 'six', 'toward', 'two', 'himself', 'nevertheless', 'could', 'to', 'whereby', 'ourselves', 'sometime', 'alone', 'always', 'after', 'must', 'one', 'why', 'both', '‘d', 'became', 'them', 'that', 'along', 'three', 'hence', 'neither', 'no', 'whence', 'none', 'anyhow', 'those', 'even', 'all', 'whenever', 'i', 'me', 'regarding', 'about', 'becoming', 'call', 'herein', 'been', '‘ve', 'enough', 'without', 'thus', 'against', 'some', 'four', 'are', 'under', 'empty', 'another', 'much', 'first', 'onto', 'almost', '’ll', 'over', '‘m', 'seem', 'really', 'hundred', 'towards', 'less', 'just', 'then', 'beyond', 'n’t', 'ten', 'indeed', 'as', 'into', 'together', 'five', 'last', 'other', 'give', 'yet', 'with', 'thru', \"'ll\", 'few', 'every', \"'s\", '‘s', 'there', 'am', 'ever', 're', 'perhaps', 'mostly', 'yours', 'in', 'twenty', 'amongst', 'either', 'fifty', 'of', 'everyone', 'during', 'hereby', 'does', 'amount', 'back', 'therein', 'whoever', 'next', 'well', 'side', 'via', 'twelve', '’re', 'because', 'until', 'have', 'never', 'if', 'hers', 'serious', '‘ll', 'yourselves', \"'re\", 'be', 'between', 'can', 'hereupon', 'our', 'they', 'unless', 'would', 'out', '‘re', 'bottom', '’s', 'everywhere', 'being', 'say', 'least', 'doing', 'which', 'same', 'several', 'whether', 'upon', 'nowhere', 'whither', 'already', 'what', 'full', 'part', 'others', 'yourself', 'anyway', '’d', 'move', 'now', 'sometimes', 'will', 'do', 'up', 'for', '’ve', 'whereafter', 'although', 'rather', 'own', 'when', 'fifteen', '’m', 'any', 'becomes', \"n't\", 'nobody', 'above', 'meanwhile', 'here', 'once', 'become', 'keep', 'herself', 'thereupon', 'whole', 'not', 'latter', 'my', 'somewhere', 'their', 'this', 'also', 'very', 'whose', 'or', 'hereafter', 'see', 'top', 'various', 'most', 'the', 'did', 'forty', 'n‘t', 'anything', 'itself', 'someone', 'due', \"'m\", 'eight', 'many', 'may', 'down', 'while', 'used', 'often', 'former', 'latterly', 'go', 'these', 'below', 'make', 'myself', 'only', 'beforehand', 'thereafter', 'nor', 'else', 'should', 'third', 'formerly', 'something', 'eleven', 'its', 'show', 'wherein', 'wherever', 'whereupon', 'us', 'through', 'themselves', 'still', 'anywhere', 'you', 'might', 'your', 'who', 'except', 'him', 'using', 'it', 'take', 'somehow', 'done', 'on', \"'ve\", 'where', 'within', 'besides', 'put', 'made', 'each', 'too', 'thereby', 'cannot', 'namely', 'nothing', 'nine', 'throughout', 'sixty', 'further', 'but', 'seemed', 'off', 'so', 'elsewhere', 'again'}\n"
     ]
    }
   ],
   "source": [
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69e18db8-7b2c-4cf7-9f84-9273846d90c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"hello\" in STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca6e24ea-5018-4026-ab9e-b1ec1de4e0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "am\n",
      "to\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Hello, I am going to McDonalds.\")\n",
    "for token in doc:\n",
    "    if nlp.vocab[token.text].is_stop:\n",
    "        print(token.text)\n",
    "    # else:\n",
    "    #     print(\"not stop-words: \", token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae9519a-8ac5-46e6-90d4-cc7d87038199",
   "metadata": {},
   "source": [
    "#### Named "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
